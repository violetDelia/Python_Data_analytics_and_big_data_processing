#!/usr/bin/python
# -*- coding: UTF-8 -*-

from pyspark import SparkContext

sc = SparkContext()
rdd1 = sc.parallelize(["Spark", "Spark", "hadoop", "hadoop", "hadoop", "hive"])
rdd2 = rdd1.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).collect()
[print("当前元素是： ", item) for item in rdd2]
