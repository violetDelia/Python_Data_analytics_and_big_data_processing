#!/usr/bin/python
# -*- coding: UTF-8 -*-

from pyspark import SparkContext

sc = SparkContext()
rdd1 = sc.parallelize(["Spark", "hadoop", "hive"])
rdd2 = sc.parallelize(["Spark", "kafka", "hbase"])
rdd3 = rdd1.union(rdd2).collect()
print("合并结果： ", rdd3)


