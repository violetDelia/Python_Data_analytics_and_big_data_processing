# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StringType, StructType

spark = SparkSession.builder.getOrCreate()
file = "hdfs://localhost:9000/input/product_detail.txt"
rdd = spark.sparkContext.textFile(file)

def convert_data(line):
    lines = line.split(",")
    # 返回的三个数据分别是：商品，单价，已售数量

    if lines[3] is None or len(lines[3]) == 0:
        price = 0
    else:
        price = float(lines[3])

    if lines[4] is None or len(lines[4]) == 0:
        sold = 0
    else:
        sold = int(lines[4])
        
    return lines[2], price, sold


# 对数据去重然后排序
data = rdd.map(lambda line: convert_data(line)).distinct(). \
    sortBy(lambda x: x[2], ascending=False).collect()

# 将rdd构造成dataframe，并调用show方法显示
schemaString = "name price sold"

fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]
schema = StructType(fields)
# 输出销量前10的商品
df = spark.createDataFrame(data, schema).limit(10)
df.show()


