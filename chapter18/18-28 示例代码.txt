# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.functions import monotonically_increasing_id
from pyspark.sql.types import StructField, StringType, StructType

spark = SparkSession.builder.getOrCreate()
file = "hdfs://localhost:9000/input/product_detail.txt"
rdd = spark.sparkContext.textFile(file)


def convert_data(line):
    lines = line.split(",")
    # 返回的三个数据分别是：销量，评分

    if lines[4] is None or len(lines[4]) == 0:
        sold = 0
    else:
        sold = int(lines[4])

    if lines[5] is None or len(lines[5]) == 0:
        score = 0
    else:
        score = float(lines[5])

    return sold, score


# 对数据去重然后排序
data = rdd.map(lambda line: convert_data(line)).distinct()

# 将rdd构造成dataframe，并调用show方法显示
schemaString = "sold score"

fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]
schema = StructType(fields)
df = spark.createDataFrame(data, schema)
df.withColumn("id", monotonically_increasing_id())
conn_param = {}
conn_param["user"] = "****"
conn_param["password"] = "****"
conn_param["driver"] = "com.mysql.jdbc.Driver"
df.write.jdbc("jdbc:mysql://localhost:3306/test", "product_info", "overwrite", conn_param)


